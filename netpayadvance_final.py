# -*- coding: utf-8 -*-
"""Netpayadvance_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dZI2ngiqd3XGBTvGxNCEkBiZPfYieo8_
"""

from google.colab import drive
drive.mount('/content/gdrive')

cd /content/gdrive/My Drive/Colab Notebooks/Netpayadvance

"""**Part1. Data pre-processing**

***1.1 data validation***
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
plt.style.use('ggplot')

import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

df_train = pd.read_csv('2018-05-17 - Recruit Sample Data Train.csv')

df_train.head(5)

# replace space with _ in column names
df_train.columns = df_train.columns.str.replace(' ','_')

df_train.shape

df_train.info()

# check missing value
null_count = pd.DataFrame(df_train.isnull().sum().sort_values(ascending=False))
null_count

# remove unique variables from dataframe
unique_fea = [column for column in df_train.columns if df_train[column].nunique() == df_train.shape[0]]
print("Remove features that are unique for each row is: ", unique_fea)

df_train = df_train.drop(columns=unique_fea, axis=1)


# identify constant variables from dataframe
constant_fea = [column for column in df_train.columns if df_train[column].nunique() < 3]
print("Explore features that are constant for all row is: ", constant_fea)

df_train['State'].value_counts()

df_train['Rent_or_Own'].value_counts()

df_train['First_Payment_Default'].value_counts()

# define target

df_train['1stdefault_binary'] = df_train['First_Payment_Default'].apply(lambda x: '1' if x is True else '0')

default_rate = pd.DataFrame(df_train['1stdefault_binary'].value_counts(normalize=True))
default_rate = default_rate.rename(columns={'1stdefault_binary':'1stdefault_rate'})


a = df_train['1stdefault_binary'].value_counts(normalize=True).tolist()
ax = default_rate.plot(kind='bar', color='lightblue')
ax.set_ylim([0,1.1])
plt.ylabel('rate', fontsize=16)
plt.xlabel('Is_1stdefault',fontsize=16)
plt.xticks(rotation= '0')
plt.xticks([0, 1], ['no', 'yes'])
plt.figure(figsize=(10,8))
for i, v in enumerate(a):
  ax.text(i-0.1,v+0.05, '{:.2f} %'.format(100*a[i]) , color='black', fontweight='bold')

# print("There are {:.2f}% customer default the first payment".format(100 * (df_train['1stdefault_binary'].value_counts(normalize=True).tolist()[1])))

# identify numerical and categorical features

numerial_fea = df_train.select_dtypes(include=[np.number]).columns.tolist()

categorical_fea = df_train.select_dtypes(include=['object', 'category']).columns.tolist()

print('no. of numerical features: ', len(numerial_fea))

print('no. of categorical features: ', len(categorical_fea))

"""***1.2. numerical features***"""

print(numerial_fea)

df_train[numerial_fea].describe()

def numerical_plot(df, colname):
    f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,4), dpi=90)
    sns.distplot(df[colname], color="r", kde=True, ax=ax1)
    ax1.set_ylabel('frequency')
    
    
    data = df.groupby(colname)['First_Payment_Default'].value_counts(normalize=True).to_frame('proportion').reset_index()        
    sns.boxplot(y = colname, x = 'First_Payment_Default', data = data, ax=ax2)
    ax2.set_ylabel(colname)
    ax2.set_xlabel('First_Payment_Default')
    sns.set(font_scale=1)

# 'Monthly_Net_Income'

numerical_plot(df_train, 'Monthly_Net_Income')

# 'Paycheck_Net_Income'

numerical_plot(df_train, 'Paycheck_Net_Income')

# 'Months_at_Residence'

numerical_plot(df_train, 'Months_at_Residence')

# 'Bank_Account_Months'

numerical_plot(df_train, 'Bank_Account_Months')

# 'Loan_Amount'

numerical_plot(df_train, 'Loan_Amount')

# plots show skew distribution, need to transformation

df_train['Monthly_Net_Income_log'] = np.log1p(df_train['Monthly_Net_Income'])
df_train['Paycheck_Net_Income_log'] = np.log1p(df_train['Paycheck_Net_Income'])
df_train['Months_at_Residence_log'] = np.log1p(df_train['Months_at_Residence'] )
df_train['Bank_Account_Months_log'] = np.log1p(df_train['Bank_Account_Months'] )

"""***1.3. categorical features ***"""

print(categorical_fea)

# build function to visualize distribution of categorical feature, and its effects on the distribution of label 'apply'

import seaborn as sns

def catogorical_plot(df, colname):
    f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,4), dpi=90)
    sns.countplot(df[colname], order=sorted(df[colname].unique()), ax=ax1)
    plt.xticks(rotation = 90)
    ax1.set_ylabel(colname)
    
    
    data = df.groupby(colname)['First_Payment_Default'].value_counts(normalize=True).to_frame('proportion').reset_index()        
    sns.barplot(x = colname, y = 'proportion', hue= "First_Payment_Default", data = data, saturation=1, ax=ax2)
    ax2.set_ylabel('1st_pymnt_default_rate over '+ colname)
    ax2.set_xlabel(colname)
    ax2.set_ylim([0,1.1])
    plt.xticks(rotation = 0)
    sns.set(font_scale=1)

# 'State'

catogorical_plot(df_train, 'State')

# 'Rent_or_Own'

catogorical_plot(df_train, 'Rent_or_Own')

# 'Pay_Cycle'

catogorical_plot(df_train, 'Pay_Cycle')

"""***1.4  datetime features***"""

import datetime
import dateutil
from dateutil import parser

#truncate timestamp into dates
df_train['Time_of_Application'] = df_train['Time_of_Application'].apply(lambda x: dateutil.parser.parse(x).date())

print("the application date start since: ", df_train['Time_of_Application'].min())
print("the application date end at: ", df_train['Time_of_Application'].max())

print("the loan due date start since: ", df_train['Loan_Due_Date'].min())
print("the loan due date end at: ", df_train['Loan_Due_Date'].max())

# convert string into datetime type
df_train['Time_of_Application'] = pd.to_datetime(df_train['Time_of_Application'])
df_train['Loan_Funded_Date'] = pd.to_datetime(df_train['Loan_Funded_Date'])
df_train['Loan_Due_Date'] = pd.to_datetime(df_train['Loan_Due_Date'])

#df_train.info()

df_timestamp = df_train[['Time_of_Application', 'Loan_Funded_Date', 'Loan_Due_Date']].copy()
df_timestamp['count'] = 1

df_timestamp = df_timestamp.set_index('Time_of_Application')
df_timestamp['count'].resample('1D').sum().plot()
plt.ylabel('Counts')
plt.title('loan application counts per day')

df_timestamp = df_timestamp.set_index('Loan_Funded_Date')
df_timestamp['count'].resample('1D').sum().plot()
plt.ylabel('Counts')
plt.title('loan funded counts per day')

df_timestamp = df_timestamp.set_index('Loan_Due_Date')
df_timestamp['count'].resample('1D').sum().plot()
plt.ylabel('Counts')
plt.title('loan due counts per day')

# generate feature showing length of loan approval after application

df_train['appl_funded_gap'] = (df_train['Loan_Funded_Date'] - df_train['Time_of_Application']).dt.days

df_train['appl_funded_gap'].value_counts().sort_values().plot(kind='bar')
plt.xlabel('days between application and funded')
plt.ylabel('counts')

# 'appl_funded_gap'

numerical_plot(df_train, 'appl_funded_gap')

df_train['Isfund_sameday'] = df_train['appl_funded_gap'].apply(lambda x: 'Yes' if x == 0 else 'No')

# 'Isfund_sameday'
catogorical_plot(df_train, 'Isfund_sameday')

# generate feature showing loan term length

df_train['loan_length'] = (df_train['Loan_Due_Date'] - df_train['Loan_Funded_Date']).dt.days
df_train['loan_length'].value_counts().sort_values().plot(kind='bar')
plt.xlabel('loan_length(day)')
plt.ylabel('counts')

# 'loan_length'

numerical_plot(df_train, 'loan_length')

"""**Part2. Exploratory data analysis **

***2.1 Visualize the relationship of target-variable, variable-variable***
"""

# identify numerical and categorical features

numerial_fea = df_train.select_dtypes(include=[np.number]).columns.tolist()
print(numerial_fea)

numerical_list = ['Monthly_Net_Income', 'Paycheck_Net_Income', 'Months_at_Residence', 'Bank_Account_Months', 'Loan_Amount', 
                  'appl_funded_gap', 'loan_length']

y = df_train['1stdefault_binary']
X = df_train[numerical_list]

X = (X-X.mean())/(X.std())

df = pd.concat([y, X], axis=1)
df = pd.melt(df, id_vars='1stdefault_binary',
            var_name='selected_features',
            value_name='value')
plt.figure(figsize=(16,7))
sns.boxplot(x='selected_features', y='value', hue='1stdefault_binary', data=df)

original_feature = ['1stdefault_binary',
                    'Monthly_Net_Income', 'Paycheck_Net_Income', 'Months_at_Residence', 'Bank_Account_Months', 'Loan_Amount', 
                    'appl_funded_gap', 'loan_length']
df2 = df_train[original_feature]
df2['1stdefault_binary'] = df2['1stdefault_binary'].astype(int)

correlation = df2.corr()
plt.figure(figsize=(20, 20))
sns.heatmap(correlation, annot=True)
plt.show()



corr_table = pd.DataFrame(correlation['1stdefault_binary'].sort_values(ascending=False).to_frame('corr_value').reset_index())
corr_table.rename(columns={'index':'name'}, inplace=True)
corr_table

sns.pairplot(df2, kind='scatter')
plt.figure(figsize=(8, 8))
plt.show()

"""***2.2 save features for modeling***"""

df_train.columns

categorical_fea = df_train.select_dtypes(include=['object', 'category']).columns.tolist()

print(categorical_fea)

# encode categorical feature to numeric values

categorical_fea = ['State', 'Rent_or_Own', 'Pay_Cycle', 'Isfund_sameday']

df_dummies = pd.get_dummies(df_train[categorical_fea], columns=categorical_fea)

df_train = df_train.join(df_dummies)

print("after get_dummy, the size of dataframe is: {}".format(df_train.shape))

df_train.columns

selected_columns = ['1stdefault_binary',
                    'Monthly_Net_Income', 'Paycheck_Net_Income', 'Months_at_Residence', 'Bank_Account_Months', 'Loan_Amount',
                    'Monthly_Net_Income_log', 'Paycheck_Net_Income_log', 'Months_at_Residence_log', 'Bank_Account_Months_log', 
                    'appl_funded_gap','loan_length',
                    'State_CA', 'State_TX','Rent_or_Own_O', 'Rent_or_Own_R', 'Pay_Cycle_BiMonthly','Pay_Cycle_BiWeekly', 
                    'Pay_Cycle_Monthly', 'Pay_Cycle_Weekly', 'Isfund_sameday_No', 'Isfund_sameday_Yes']

# save data for modeling.

df_train_select = df_train[selected_columns]
df_train_select.to_csv('Train_model.csv', index=False)

"""**Part3. Biuild classfication models**

***3.1 Load data***
"""

import pandas as pd

df = pd.read_csv('Train_model.csv')

df.head()

df.shape

df.info()

selected_columns = ['Loan_Amount',
                    'Monthly_Net_Income_log', 'Paycheck_Net_Income_log', 'Months_at_Residence_log', 'Bank_Account_Months_log', 
                    'appl_funded_gap','loan_length',
                    'State_CA', 'State_TX','Rent_or_Own_O', 'Rent_or_Own_R', 'Pay_Cycle_BiMonthly','Pay_Cycle_BiWeekly', 
                    'Pay_Cycle_Monthly', 'Pay_Cycle_Weekly', 'Isfund_sameday_No', 'Isfund_sameday_Yes']

feat_name = selected_columns

feat_name = selected_columns

X = df[selected_columns].values
y = df['1stdefault_binary'].values

X.shape

y

"""***3.2 split train and test dataset***"""

# import train test split function from sklearn
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

# scale variables
from sklearn import preprocessing
X_train = preprocessing.scale(X_train)
X_test  = preprocessing.scale(X_test)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, roc_auc_score

def plot_roc_curve(y_train, y_train_pred, y_test, y_test_pred):
    roc_auc_train = roc_auc_score(y_train, y_train_pred)
    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred)

    roc_auc_test = roc_auc_score(y_test, y_test_pred)
    fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred)
    plt.figure()
    lw = 2
    plt.plot(fpr_train, tpr_train, color='green',
             lw=lw, label='ROC Train (AUC = %0.4f)' % roc_auc_train)
    plt.plot(fpr_test, tpr_test, color='darkorange',
             lw=lw, label='ROC Test (AUC = %0.4f)' % roc_auc_test)
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()

def get_performance_metrics(y_train, y_train_pred, y_test, y_test_pred, threshold=0.5):
    metric_names = ['AUC','Accuracy','Precision','Recall','f1-score']
    metric_values_train = [roc_auc_score(y_train, y_train_pred),
                    accuracy_score(y_train, y_train_pred>threshold),
                    precision_score(y_train, y_train_pred>threshold),
                    recall_score(y_train, y_train_pred>threshold),
                    f1_score(y_train, y_train_pred>threshold)
                   ]
    metric_values_test = [roc_auc_score(y_test, y_test_pred),
                    accuracy_score(y_test, y_test_pred>threshold),
                    precision_score(y_test, y_test_pred>threshold),
                    recall_score(y_test, y_test_pred>threshold),
                    f1_score(y_test, y_test_pred>threshold)
                   ]
    all_metrics = pd.DataFrame({'metrics':metric_names,
                                'train':metric_values_train,
                                'test':metric_values_test},columns=['metrics','train','test']).set_index('metrics')
    print(all_metrics)

def train_test_model(clf, X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)

    y_train_pred = clf.predict(X_train)
    p_train_pred = clf.predict_proba(X_train)[:,1]

    y_test_pred = clf.predict(X_test)
    p_test_pred = clf.predict_proba(X_test)[:,1]

    get_performance_metrics(y_train, p_train_pred, y_test, p_test_pred)
    plot_roc_curve(y_train, p_train_pred, y_test, p_test_pred)

"""***3.3 Logistic regression***"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.linear_model import LogisticRegression
# 
# 
# clf = LogisticRegression(C=1.0, penalty='l2')
# 
# LRmodel = clf.fit(X_train, y_train)
# 
# train_test_model(clf, X_train, y_train, X_test, y_test)

df_coeffs = pd.DataFrame(list(zip(feat_name, clf.coef_.flatten()))).sort_values(by=[1], ascending=False)
df_coeffs.columns = ['feature', 'coeff']
df_coeffs

ax = df_coeffs.plot.barh()
t = np.arange(X.shape[1])
ax.set_yticks(t)
ax.set_yticklabels(df_coeffs['feature'])
plt.show()

import pickle
filename = '/content/gdrive/My Drive/Colab Notebooks/Netpayadvance/LR_model.sav'
pickle.dump(LRmodel, open(filename, 'wb'))

"""***3.4 Random forest***"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import RandomForestClassifier
# 
# parameters = {'n_estimators': 50,
#               'max_features': 'auto',
#               'criterion': 'gini',
#               'max_depth': 20,
#               'min_samples_split': 5,
#               'min_samples_leaf': 40,
#               'random_state': 0,
#               'n_jobs': -1}
# 
# clf = RandomForestClassifier(**parameters)
# 
# clf.fit(X_train, y_train)
# 
# rfmodel = clf.fit(X_train, y_train)
# 
# train_test_model(clf, X_train, y_train, X_test, y_test)

feature_importances_RF = pd.DataFrame(clf.feature_importances_,
                                   index = feat_name,
                                    columns=['importance']).sort_values('importance', ascending=False)
feature_importances_RF.plot(kind='bar', figsize=(10, 4), title= 'feature importance')
plt.ylabel('feature importance based on Random Forest', fontsize=16)
plt.xlabel('feature names', fontsize=16)

"""***3.5 Gradient boost***"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import GradientBoostingClassifier
# 
# parameters = {'n_estimators': 50,
#     'max_depth': 2,
#     'learning_rate': 0.2,
#     'random_state': 42,
#     'subsample': 0.7,
#     'max_features':0.8}
# 
# 
# 
# clf = GradientBoostingClassifier(**parameters)
# 
# train_test_model(clf, X_train, y_train, X_test, y_test)

"""***3.6 SVM***"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from sklearn.svm import SVC
# 
# parameters = {'probability':True, 
#     'max_iter':2000}
#     
# clf = SVC(**parameters)    
# 
# train_test_model(clf, X_train, y_train, X_test, y_test)

"""***3.7 Neural network***"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.neural_network import MLPClassifier
# 
# 
# parameters = {
#     'solver':'adam', 
#     'activation':'relu',
#     'alpha':1e-5, 
#     'hidden_layer_sizes':(5,5), 
#     'learning_rate':'adaptive',
#     'random_state':1
#     }
# clf = MLPClassifier(**parameters)
# 
# 
# train_test_model(clf, X_train, y_train, X_test, y_test)

"""**Part4. Hyperparameter tuning**

***4.1 Grid search tuning hyperparameter on Random forest***
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score
# from sklearn.model_selection import GridSearchCV
# 
# clf = RandomForestClassifier()
# 
# 
# param_grid = {'n_estimators': [50,100,200,300], 
#               'max_features': ['auto'], 
#               'criterion': ['gini'],
#               'max_depth': [10,20,40], 
#               'min_samples_split': [5, 10],
#               'min_samples_leaf': [2,10,20],
#               'n_jobs':[-1]
#              }
# 
# 
# acc_scorer = make_scorer(roc_auc_score)
# 
# 
# grid_obj = GridSearchCV(clf, param_grid, cv=5, scoring=acc_scorer)
# grid_obj = grid_obj.fit(X_train, y_train)
# 
# 
# RFtuned = clf = grid_obj.best_estimator_
# 
# clf.fit(X_train, y_train)

train_test_model(clf, X_train, y_train, X_test, y_test)

import pickle
filename = '/content/gdrive/My Drive/Colab Notebooks/Netpayadvance/RFtuned_model.sav'
pickle.dump(clf, open(filename, 'wb'))

"""***4.2 Build LightGBM with cross validation***"""

import pandas as pd
import time
import lightgbm as lgb
from sklearn.model_selection import KFold,StratifiedKFold
from sklearn.model_selection import GridSearchCV

df = pd.read_csv('Train_model.csv')


selected_columns = ['Loan_Amount',
                    'Monthly_Net_Income_log', 'Paycheck_Net_Income_log', 'Months_at_Residence_log', 'Bank_Account_Months_log', 
                    'appl_funded_gap','loan_length',
                    'State_CA', 'State_TX','Rent_or_Own_O', 'Rent_or_Own_R', 'Pay_Cycle_BiMonthly','Pay_Cycle_BiWeekly', 
                    'Pay_Cycle_Monthly', 'Pay_Cycle_Weekly', 'Isfund_sameday_No', 'Isfund_sameday_Yes']

X = df[selected_columns]
y = df['1stdefault_binary']

n_fold = 5
folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)

params = {'objective': 'binary',
         'learning_rate': 0.05,
         'boosting': 'gbdt',
         'bagging_fraction': 0.5,
         'feature_fraction': 0.3,
         'bagging_seed': 11,
         'random_state': 42,
         'metric': 'auc',
         'verbosity': -1,
         'subsample': 0.5}

prediction = np.zeros(len(X_test))
for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):
    print('Fold', fold_n, 'started at', time.ctime())
    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]
    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]
    
    train_data = lgb.Dataset(X_train, label=y_train)
    valid_data = lgb.Dataset(X_valid, label=y_valid)
        
    lgb_model = lgb.train(params,train_data,num_boost_round=30000,
                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 100)
            
    prediction += lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)/5

import pickle
filename = '/content/gdrive/My Drive/Colab Notebooks/Netpayadvance/lgb_model.sav'
pickle.dump(lgb_model, open(filename, 'wb'))

"""**Part5. Predicting loan status on test data**

***5.1 proceed Test data with pipline above***
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

import copy


df_test = pd.read_csv('2018-05-17 - Recruit Sample Data Test.csv')
df_test_copy = copy.deepcopy(df_test)

# replace space with _ in column names
df_test.columns = df_test.columns.str.replace(' ','_')

# check missing value
null_count = pd.DataFrame(df_test.isnull().sum().sort_values(ascending=False))
null_count

# identify unique variables from dataframe
unique_fea = [column for column in df_test.columns if df_test[column].nunique() == df_test.shape[0]]
print("Identify features that are unique for each row is: ", unique_fea)

# identify constant variables from dataframe
constant_fea = [column for column in df_test.columns if df_test[column].nunique() < 3]
print("Explore features that are constant for all row is: ", constant_fea)

# drop SetID
df_test = df_test.drop(columns=['SetID'], axis=1)

# transforma numerical variables
df_test['Monthly_Net_Income_log'] = np.log1p(df_test['Monthly_Net_Income'])
df_test['Paycheck_Net_Income_log'] = np.log1p(df_test['Paycheck_Net_Income'])
df_test['Months_at_Residence_log'] = np.log1p(df_test['Months_at_Residence'] )
df_test['Bank_Account_Months_log'] = np.log1p(df_test['Bank_Account_Months'] )

# datetime variables
import datetime
import dateutil
from dateutil import parser

#truncate timestamp into dates
df_test['Time_of_Application'] = df_test['Time_of_Application'].apply(lambda x: dateutil.parser.parse(x).date())

print("the application date start since: ", df_test['Time_of_Application'].min())
print("the application date end at: ", df_test['Time_of_Application'].max())
print("the loan due date start since: ", df_test['Loan_Due_Date'].min())
print("the loan due date end at: ", df_test['Loan_Due_Date'].max())


# convert string into datetime type
df_test['Time_of_Application'] = pd.to_datetime(df_test['Time_of_Application'])
df_test['Loan_Funded_Date'] = pd.to_datetime(df_test['Loan_Funded_Date'])
df_test['Loan_Due_Date'] = pd.to_datetime(df_test['Loan_Due_Date'])

# generate feature showing length of loan approval after application
df_test['appl_funded_gap'] = (df_test['Loan_Funded_Date'] - df_test['Time_of_Application']).dt.days
df_test['Isfund_sameday'] = df_test['appl_funded_gap'].apply(lambda x: 'Yes' if x == 0 else 'No')

# generate feature showing loan term length
df_test['loan_length'] = (df_train['Loan_Due_Date'] - df_train['Loan_Funded_Date']).dt.days

# encode categorical feature to numeric values
categorical_fea = ['State', 'Rent_or_Own', 'Pay_Cycle', 'Isfund_sameday']

df_dummies = pd.get_dummies(df_test[categorical_fea], columns=categorical_fea)

df_test = df_test.join(df_dummies)

df_test.columns

# save data for prediction.
selected_columns = ['Loan_Amount', 'Monthly_Net_Income_log', 'Paycheck_Net_Income_log',
       'Months_at_Residence_log', 'Bank_Account_Months_log', 'appl_funded_gap',
       'loan_length', 'State_CA', 'State_TX', 'Rent_or_Own_O', 'Rent_or_Own_R',
       'Pay_Cycle_BiMonthly', 'Pay_Cycle_BiWeekly', 'Pay_Cycle_Monthly',
       'Pay_Cycle_Weekly', 'Isfund_sameday_No', 'Isfund_sameday_Yes']


df_test_cleaned = df_test[selected_columns]
df_test_cleaned.to_csv('Test_cleaned.csv', index=False)

df_test_scaled = preprocessing.scale(df_test_cleaned)

p_test_pred_LR = LRmodel.predict_proba(df_test_scaled)[:,1]
print('Logistic regression predicts {}'.format(p_test_pred_LR.size) + ' row of probability')

p_test_pred_RF = RFtuned.predict_proba(df_test_scaled)[:,1]
print('Random forest predicts {}'.format(p_test_pred_RF.size) + ' row of probability')


p_test_pred_lgb = lgb_model.predict(df_test_cleaned , num_iteration=lgb_model.best_iteration)/5
print('LightGBM predicts {}'.format(p_test_pred_lgb.size) + ' row of probability')

LR_result = pd.DataFrame(p_test_pred_LR)
LR_result.columns=['logistic_regression_prob']


RF_result = pd.DataFrame(p_test_pred_RF)
RF_result.columns=['random_forest_prob']


lgb_result = pd.DataFrame(p_test_pred_lgb)
lgb_result.columns=['LightGBM_prob']

df_new = df_test_copy[['SetID']]

df_new01 = RF_result.join(lgb_result)

df_new02 = df_new.join(LR_result)
df_new03 = df_new02.join(df_new01)

df_new03.head(3)
df_new03.to_csv('prediction_result.csv', index=False)

